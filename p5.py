# -*- coding: utf-8 -*-
"""P5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FdJa5yGIiRb_FhxtBlUrnzE9XNRn3-zL

# U-Net
"""
"""## Model"""



import numpy as np
import torch
import torchvision.transforms.functional
from torch import nn

class DoubleConvolution(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.act1 = nn.ReLU()
        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.act2 = nn.ReLU()

    def forward(self, x: torch.Tensor):
        x = self.first(x)
        x = self.act1(x)
        x = self.second(x)
        x = self.act2(x)
        return x

class DownSample(nn.Module):
    def __init__(self):
        super().__init__()
        self.pool = nn.MaxPool2d(2)

    def forward(self, x: torch.Tensor):
        return self.pool(x)

class UpSample(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)

    def forward(self, x: torch.Tensor):
        return self.up(x)

class CropAndConcat(nn.Module):
    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):
        contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[1], x.shape[2]])
        x = torch.cat([x, contracting_x], dim=0)
        return x

class UNet(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])
        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])
        self.middle_conv = DoubleConvolution(512, 1024)
        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])
        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])
        self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])
        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)

    def forward(self, x: torch.Tensor):
        pass_through = []
        #print("----------------")
        for i in range(len(self.down_conv)):
            x = self.down_conv[i](x)
            pass_through.insert(0,x)
            #print(x.shape)
            x = self.down_sample[i](x)
            #print(x.shape)
        x = self.middle_conv(x)
        #print(x.shape)    
        #print("----------------")
        for i in range(len(self.up_conv)):
            x = self.up_sample[i](x)
            tmp = pass_through[i]
            #print(tmp.shape)
            #print(x.shape)
            x = self.concat[i](x, tmp)
            x = self.up_conv[i](x)
        x = self.final_conv(x)
        return x

"""## Data Processing"""


import os
import pandas as pd
import pydicom as dicom
import nibabel as nib
from tqdm import tqdm
import torchvision.transforms as transforms
from PIL import Image
from pathlib import Path, PurePath, PurePosixPath, PureWindowsPath
from torch.utils.data import Dataset

class PancreasCTDataset(Dataset):
    def __init__(self, annotation_file, img_dir_path, target_dir_path, transform=None, target_transform=None):
        self.sample = self.make_dataset(annotation_file, img_dir_path, target_dir_path)
        self.img_dir_path = img_dir_path
        self.target_dir_path = target_dir_path
        self.transform = transform
        self.target_transform = target_transform
        
    @staticmethod
    def make_dataset(annotation_file, img_dir_path, target_dir_path):
        dataset = []

        pancreas_location = pd.read_csv(annotation_file)[['Subject ID', 'File Location']].to_numpy()
        for pancreas_name, pancreas_path in pancreas_location:
            target_filename = 'label' + pancreas_name[-4:] + '.nii.gz'
            target_path = os.path.join(target_dir_path, target_filename)
            pancreas_path = Path(pancreas_path)
            pancreas_path = os.path.join(img_dir_path, pancreas_path)
            pancreas_slice_filename = os.listdir(pancreas_path)
            pancreas_slice_filename.sort()
            for layer, filename in enumerate(pancreas_slice_filename):
                pancreas_slice_path = os.path.join(pancreas_path, filename)
                dataset.append((pancreas_slice_path, (target_path, layer)))
        #for i in dataset:
        #    print (i)
        return dataset

    def __len__(self):
        return len(self.sample)

    def __getitem__(self, idx):
        pancreas_slice_path, target_slice_path = self.sample[idx]
        slice = dicom.dcmread(pancreas_slice_path).pixel_array
        target = np.moveaxis(nib.load(target_slice_path[0]).get_fdata(), -1, 0)[target_slice_path[1]]
        if self.transform:
            slice = self.transform(slice)
        if self.target_transform:
            target = self.target_transform(target)
        return slice, target

"""## Training"""

import torch.optim as optim

print(torch.__version__)
#t = torch.Tensor()
#print(t.dtype)
#print(t.device)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)   
print(f"Using {device} device")

model = UNet(in_channels=4, out_channels=4).to(device)
if torch.cuda.is_available():
    model.cuda()
data_path = Path(r'C:\proyecto-IA\data')

annotation_file = os.path.join(data_path, 'metadata.csv')
img_dir_path = data_path
target_dir_path = os.path.join(data_path, 'Pancreas-Labels')

import math
from torch import default_generator, randperm
from torch._utils import _accumulate
from torch.utils.data.dataset import Subset

def random_split(dataset, lengths,
                 gen    erator=default_generator):
    r"""
    Randomly split a dataset into non-overlapping new datasets of given lengths.

    If a list of fractions that sum up to 1 is given,
    the lengths will be computed automatically as
    floor(frac * len(dataset)) for each fraction provided.

    After computing the lengths, if there are any remainders, 1 count will be
    distributed in round-robin fashion to the lengths
    until there are no remainders left.

    Optionally fix the generator for reproducible results, e.g.:

    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))
    >>> random_split(range(30), [0.3, 0.3, 0.4], generator=torch.Generator(
    ...   ).manual_seed(42))

    Args:
        dataset (Dataset): Dataset to be split
        lengths (sequence): lengths or fractions of splits to be produced
        generator (Generator): Generator used for the random permutation.
    """
    if math.isclose(sum(lengths), 1) and sum(lengths) <= 1:
        subset_lengths: List[int] = []
        for i, frac in enumerate(lengths):
            if frac < 0 or frac > 1:
                raise ValueError(f"Fraction at index {i} is not between 0 and 1")
            n_items_in_split = int(
                math.floor(len(dataset) * frac)  # type: ignore[arg-type]
            )
            subset_lengths.append(n_items_in_split)
        remainder = len(dataset) - sum(subset_lengths)  # type: ignore[arg-type]
        # add 1 to all the lengths in round-robin fashion until the remainder is 0
        for i in range(remainder):
            idx_to_add_at = i % len(subset_lengths)
            subset_lengths[idx_to_add_at] += 1
        lengths = subset_lengths
        for i, length in enumerate(lengths):
            if length == 0:
                warnings.warn(f"Length of split at index {i} is 0. "
                              f"This might result in an empty dataset.")

    # Cannot verify that dataset is Sized
    if sum(lengths) != len(dataset):    # type: ignore[arg-type]
        raise ValueError("Sum of input lengths does not equal the length of the input dataset!")

    indices = randperm(sum(lengths), generator=generator).tolist()  # type: ignore[call-overload]
    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]



transform = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])

dataset = PancreasCTDataset(annotation_file=annotation_file, img_dir_path=img_dir_path, target_dir_path=target_dir_path)

trainset, valset, testset = random_split(dataset=dataset, lengths=[0.7, 0.2, 0.1], generator=torch.Generator().manual_seed(42))

batch_size = 4

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)
valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=0)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=0)

len(dataset)
len(trainset)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(tqdm(trainloader)):
        # obtener los inputs; la data es una lista de [inputs, labels]
        inputs, labels = data
        #para mantener coherencia con el resto del codigo
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        # enviando tensores de entrada a la gpu
        inputs, labels = inputs.to(torch.float32).to(device), labels.to(device)
        #print(inputs.dtype)
        #print(inputs.shape)
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize 
        outputs= model(inputs)
        #print(outputs.dtype)
       # print(outputs.shape)
        loss = criterion(outputs, labels.cuda())
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0
    
print('Finished Training')
